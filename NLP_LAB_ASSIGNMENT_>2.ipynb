{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwytZTWmOCsBDqY35hif0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gowrav31/NLP/blob/main/NLP_LAB_ASSIGNMENT_%3E2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3Mc6iLrUvmU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "956718c5",
        "outputId": "f8f5bb92-c976-4569-cfe0-8105da8d85dc"
      },
      "source": [
        "import sys\n",
        "\n",
        "# Install nltk and spacy\n",
        "!{sys.executable} -m pip install nltk spacy\n",
        "\n",
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Download spaCy model\n",
        "!{sys.executable} -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"Libraries and data downloaded successfully.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Libraries and data downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbe12f21",
        "outputId": "a554042f-5863-47e2-8984-e93978283346"
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(\"NLTK and spaCy libraries imported and spaCy model loaded successfully.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK and spaCy libraries imported and spaCy model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7e5e2bb",
        "outputId": "72c0b595-e755-46da-c2c5-31e6c4bb9baa"
      },
      "source": [
        "medical_text = \"\"\"\\nType 2 diabetes is a chronic condition that affects the way the body processes blood sugar (glucose). With type 2 diabetes, the body either doesn't produce enough insulin, or it resists insulin. Symptoms can include increased thirst, frequent urination, increased hunger, unintended weight loss, fatigue, blurred vision, slow-healing sores, and frequent infections. Long-term complications can include heart disease, stroke, kidney disease, nerve damage, and eye damage. Management often involves lifestyle changes such as diet and exercise, and sometimes medication or insulin therapy. Regular monitoring of blood glucose levels is crucial.\\n\"\"\"\n",
        "\n",
        "print(\"Medical text corpus loaded into 'medical_text' variable.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical text corpus loaded into 'medical_text' variable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf5ca5a2",
        "outputId": "710dd4a5-82f9-415a-cc53-ada2dd4c3fb5"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Tokenize the medical text into sentences\n",
        "nltk_sentences = sent_tokenize(medical_text)\n",
        "\n",
        "# Tokenize each sentence into words\n",
        "nltk_words = [word_tokenize(sentence) for sentence in nltk_sentences]\n",
        "\n",
        "# Print the first few sentences and their corresponding tokenized words\n",
        "print(\"First 3 Tokenized Sentences:\")\n",
        "for i, sentence in enumerate(nltk_sentences[:3]):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")\n",
        "    print(f\"Words {i+1}: {nltk_words[i]}\")\n",
        "    print(\"--------------------\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 Tokenized Sentences:\n",
            "Sentence 1: \n",
            "Type 2 diabetes is a chronic condition that affects the way the body processes blood sugar (glucose).\n",
            "Words 1: ['Type', '2', 'diabetes', 'is', 'a', 'chronic', 'condition', 'that', 'affects', 'the', 'way', 'the', 'body', 'processes', 'blood', 'sugar', '(', 'glucose', ')', '.']\n",
            "--------------------\n",
            "Sentence 2: With type 2 diabetes, the body either doesn't produce enough insulin, or it resists insulin.\n",
            "Words 2: ['With', 'type', '2', 'diabetes', ',', 'the', 'body', 'either', 'does', \"n't\", 'produce', 'enough', 'insulin', ',', 'or', 'it', 'resists', 'insulin', '.']\n",
            "--------------------\n",
            "Sentence 3: Symptoms can include increased thirst, frequent urination, increased hunger, unintended weight loss, fatigue, blurred vision, slow-healing sores, and frequent infections.\n",
            "Words 3: ['Symptoms', 'can', 'include', 'increased', 'thirst', ',', 'frequent', 'urination', ',', 'increased', 'hunger', ',', 'unintended', 'weight', 'loss', ',', 'fatigue', ',', 'blurred', 'vision', ',', 'slow-healing', 'sores', ',', 'and', 'frequent', 'infections', '.']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e8a1547",
        "outputId": "d07f08e2-c4ed-4d41-94e0-4d79b5ea2ca7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Tokenize the medical text into sentences\n",
        "ltk_sentences = sent_tokenize(medical_text)\n",
        "\n",
        "# Tokenize each sentence into words\n",
        "nltk_words = [word_tokenize(sentence) for sentence in nltk_sentences]\n",
        "\n",
        "# Print the first few sentences and their corresponding tokenized words\n",
        "print(\"First 3 Tokenized Sentences:\")\n",
        "for i, sentence in enumerate(nltk_sentences[:3]):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")\n",
        "    print(f\"Words {i+1}: {nltk_words[i]}\")\n",
        "    print(\"--------------------\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 Tokenized Sentences:\n",
            "Sentence 1: \n",
            "Type 2 diabetes is a chronic condition that affects the way the body processes blood sugar (glucose).\n",
            "Words 1: ['Type', '2', 'diabetes', 'is', 'a', 'chronic', 'condition', 'that', 'affects', 'the', 'way', 'the', 'body', 'processes', 'blood', 'sugar', '(', 'glucose', ')', '.']\n",
            "--------------------\n",
            "Sentence 2: With type 2 diabetes, the body either doesn't produce enough insulin, or it resists insulin.\n",
            "Words 2: ['With', 'type', '2', 'diabetes', ',', 'the', 'body', 'either', 'does', \"n't\", 'produce', 'enough', 'insulin', ',', 'or', 'it', 'resists', 'insulin', '.']\n",
            "--------------------\n",
            "Sentence 3: Symptoms can include increased thirst, frequent urination, increased hunger, unintended weight loss, fatigue, blurred vision, slow-healing sores, and frequent infections.\n",
            "Words 3: ['Symptoms', 'can', 'include', 'increased', 'thirst', ',', 'frequent', 'urination', ',', 'increased', 'hunger', ',', 'unintended', 'weight', 'loss', ',', 'fatigue', ',', 'blurred', 'vision', ',', 'slow-healing', 'sores', ',', 'and', 'frequent', 'infections', '.']\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a92870b",
        "outputId": "b2dd7d6d-b2b4-4b85-c4f7-0386d11122dd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Tokenize the medical text into sentences\n",
        "nltk_sentences = sent_tokenize(medical_text)\n",
        "\n",
        "# Tokenize each sentence into words\n",
        "nltk_words = [word_tokenize(sentence) for sentence in nltk_sentences]\n",
        "\n",
        "# Print the first few sentences and their corresponding tokenized words\n",
        "print(\"First 3 Tokenized Sentences:\")\n",
        "for i, sentence in enumerate(nltk_sentences[:3]):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")\n",
        "    print(f\"Words {i+1}: {nltk_words[i]}\")\n",
        "    print(\"--------------------\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 Tokenized Sentences:\n",
            "Sentence 1: \n",
            "Type 2 diabetes is a chronic condition that affects the way the body processes blood sugar (glucose).\n",
            "Words 1: ['Type', '2', 'diabetes', 'is', 'a', 'chronic', 'condition', 'that', 'affects', 'the', 'way', 'the', 'body', 'processes', 'blood', 'sugar', '(', 'glucose', ')', '.']\n",
            "--------------------\n",
            "Sentence 2: With type 2 diabetes, the body either doesn't produce enough insulin, or it resists insulin.\n",
            "Words 2: ['With', 'type', '2', 'diabetes', ',', 'the', 'body', 'either', 'does', \"n't\", 'produce', 'enough', 'insulin', ',', 'or', 'it', 'resists', 'insulin', '.']\n",
            "--------------------\n",
            "Sentence 3: Symptoms can include increased thirst, frequent urination, increased hunger, unintended weight loss, fatigue, blurred vision, slow-healing sores, and frequent infections.\n",
            "Words 3: ['Symptoms', 'can', 'include', 'increased', 'thirst', ',', 'frequent', 'urination', ',', 'increased', 'hunger', ',', 'unintended', 'weight', 'loss', ',', 'fatigue', ',', 'blurred', 'vision', ',', 'slow-healing', 'sores', ',', 'and', 'frequent', 'infections', '.']\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "48554084",
        "outputId": "50a5a96f-b44c-439a-f4f4-e8cceff3ccb8"
      },
      "source": [
        "## Tokenize Sentences and Words (spaCy)\n",
        "\n",
        "### Subtask:\n",
        "Tokenize the loaded medical text into sentences and then into words using spaCy's processing capabilities."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 4) (ipython-input-3875437367.py, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3875437367.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Tokenize the loaded medical text into sentences and then into words using spaCy's processing capabilities.\u001b[0m\n\u001b[0m                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "3f31ab5b",
        "outputId": "2449e52e-a028-4b06-9690-7f947d25f9f7"
      },
      "source": [
        "## Tokenize Sentences and Words (spaCy)\n",
        "\n",
        "### Subtask:\n",
        "Tokenize the loaded medical text into sentences and then into words using spaCy's processing capabilities."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 4) (ipython-input-3875437367.py, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3875437367.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Tokenize the loaded medical text into sentences and then into words using spaCy's processing capabilities.\u001b[0m\n\u001b[0m                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ded3cd",
        "outputId": "ec3d40a1-e16e-433f-98bb-adb27df2ec20"
      },
      "source": [
        "# Process the medical text with spaCy\n",
        "doc = nlp(medical_text)\n",
        "\n",
        "# Tokenize into sentences (spaCy's doc.sents provides sentence spans)\n",
        "spacy_sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "# Tokenize each sentence into words\n",
        "spacy_words = [[token.text for token in sent] for sent in doc.sents]\n",
        "\n",
        "# Print the first few sentences and their corresponding tokenized words\n",
        "print(\"First 3 Tokenized Sentences (spaCy):\")\n",
        "for i, sentence in enumerate(spacy_sentences[:3]):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")\n",
        "    print(f\"Words {i+1}: {spacy_words[i]}\")\n",
        "    print(\"--------------------\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 Tokenized Sentences (spaCy):\n",
            "Sentence 1: \n",
            "Type 2 diabetes is a chronic condition that affects the way the body processes blood sugar (glucose).\n",
            "Words 1: ['\\n', 'Type', '2', 'diabetes', 'is', 'a', 'chronic', 'condition', 'that', 'affects', 'the', 'way', 'the', 'body', 'processes', 'blood', 'sugar', '(', 'glucose', ')', '.']\n",
            "--------------------\n",
            "Sentence 2: With type 2 diabetes, the body either doesn't produce enough insulin, or it resists insulin.\n",
            "Words 2: ['With', 'type', '2', 'diabetes', ',', 'the', 'body', 'either', 'does', \"n't\", 'produce', 'enough', 'insulin', ',', 'or', 'it', 'resists', 'insulin', '.']\n",
            "--------------------\n",
            "Sentence 3: Symptoms can include increased thirst, frequent urination, increased hunger, unintended weight loss, fatigue, blurred vision, slow-healing sores, and frequent infections.\n",
            "Words 3: ['Symptoms', 'can', 'include', 'increased', 'thirst', ',', 'frequent', 'urination', ',', 'increased', 'hunger', ',', 'unintended', 'weight', 'loss', ',', 'fatigue', ',', 'blurred', 'vision', ',', 'slow', '-', 'healing', 'sores', ',', 'and', 'frequent', 'infections', '.']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9a3aa84",
        "outputId": "997c5ea3-9ed0-4f89-ea04-0afdab1f79d3"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Instantiate the Porter Stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Apply stemming to the NLTK-tokenized words\n",
        "nltk_stemmed_words = []\n",
        "for sentence_words in nltk_words:\n",
        "    stemmed_sentence = [porter_stemmer.stem(word) for word in sentence_words]\n",
        "    nltk_stemmed_words.append(stemmed_sentence)\n",
        "\n",
        "# Print the first few stemmed sentences\n",
        "print(\"First 3 Stemmed Sentences (NLTK Porter Stemmer):\")\n",
        "for i, stemmed_sentence in enumerate(nltk_stemmed_words[:3]):\n",
        "    print(f\"Sentence {i+1}: {stemmed_sentence}\")\n",
        "    print(\"--------------------\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 Stemmed Sentences (NLTK Porter Stemmer):\n",
            "Sentence 1: ['type', '2', 'diabet', 'is', 'a', 'chronic', 'condit', 'that', 'affect', 'the', 'way', 'the', 'bodi', 'process', 'blood', 'sugar', '(', 'glucos', ')', '.']\n",
            "--------------------\n",
            "Sentence 2: ['with', 'type', '2', 'diabet', ',', 'the', 'bodi', 'either', 'doe', \"n't\", 'produc', 'enough', 'insulin', ',', 'or', 'it', 'resist', 'insulin', '.']\n",
            "--------------------\n",
            "Sentence 3: ['symptom', 'can', 'includ', 'increas', 'thirst', ',', 'frequent', 'urin', ',', 'increas', 'hunger', ',', 'unintend', 'weight', 'loss', ',', 'fatigu', ',', 'blur', 'vision', ',', 'slow-heal', 'sore', ',', 'and', 'frequent', 'infect', '.']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8876caa",
        "outputId": "a3fd9713-6f50-4b3b-f4a2-ebf305ec94ce"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Instantiate the WordNet Lemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to the NLTK-tokenized words\n",
        "nltk_lemmatized_words = []\n",
        "for sentence_words in nltk_words:\n",
        "    lemmatized_sentence = [wordnet_lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "    nltk_lemmatized_words.append(lemmatized_sentence)\n",
        "\n",
        "# Print the first few lemmatized sentences\n",
        "print(\"First 3 Lemmatized Sentences (NLTK WordNet Lemmatizer):\")\n",
        "for i, lemmatized_sentence in enumerate(nltk_lemmatized_words[:3]):\n",
        "    print(f\"Sentence {i+1}: {lemmatized_sentence}\")\n",
        "    print(\"--------------------\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 Lemmatized Sentences (NLTK WordNet Lemmatizer):\n",
            "Sentence 1: ['Type', '2', 'diabetes', 'is', 'a', 'chronic', 'condition', 'that', 'affect', 'the', 'way', 'the', 'body', 'process', 'blood', 'sugar', '(', 'glucose', ')', '.']\n",
            "--------------------\n",
            "Sentence 2: ['With', 'type', '2', 'diabetes', ',', 'the', 'body', 'either', 'doe', \"n't\", 'produce', 'enough', 'insulin', ',', 'or', 'it', 'resists', 'insulin', '.']\n",
            "--------------------\n",
            "Sentence 3: ['Symptoms', 'can', 'include', 'increased', 'thirst', ',', 'frequent', 'urination', ',', 'increased', 'hunger', ',', 'unintended', 'weight', 'loss', ',', 'fatigue', ',', 'blurred', 'vision', ',', 'slow-healing', 'sore', ',', 'and', 'frequent', 'infection', '.']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34d1465f",
        "outputId": "95d3e231-7199-41c0-ef24-a35e632ed956"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Ensure the spaCy model is loaded (it was loaded in a previous step)\n",
        "# nlp = spacy.load(\"en_core_web_sm\") # Assuming nlp is already loaded\n",
        "\n",
        "# Process the medical text with spaCy (if not already done in the current context)\n",
        "doc = nlp(medical_text)\n",
        "\n",
        "# Apply lemmatization to the spaCy-tokenized words\n",
        "spacy_lemmatized_words = []\n",
        "for sent in doc.sents:\n",
        "    lemmatized_sentence = [token.lemma_ for token in sent]\n",
        "    spacy_lemmatized_words.append(lemmatized_sentence)\n",
        "\n",
        "# Print the first few lemmatized sentences\n",
        "print(\"First 3 Lemmatized Sentences (spaCy):\")\n",
        "for i, lemmatized_sentence in enumerate(spacy_lemmatized_words[:3]):\n",
        "    print(f\"Sentence {i+1}: {lemmatized_sentence}\")\n",
        "    print(\"--------------------\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 Lemmatized Sentences (spaCy):\n",
            "Sentence 1: ['\\n', 'Type', '2', 'diabetes', 'be', 'a', 'chronic', 'condition', 'that', 'affect', 'the', 'way', 'the', 'body', 'process', 'blood', 'sugar', '(', 'glucose', ')', '.']\n",
            "--------------------\n",
            "Sentence 2: ['with', 'type', '2', 'diabetes', ',', 'the', 'body', 'either', 'do', 'not', 'produce', 'enough', 'insulin', ',', 'or', 'it', 'resist', 'insulin', '.']\n",
            "--------------------\n",
            "Sentence 3: ['symptom', 'can', 'include', 'increase', 'thirst', ',', 'frequent', 'urination', ',', 'increase', 'hunger', ',', 'unintended', 'weight', 'loss', ',', 'fatigue', ',', 'blurred', 'vision', ',', 'slow', '-', 'heal', 'sore', ',', 'and', 'frequent', 'infection', '.']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b139b81a",
        "outputId": "3264f9b4-78fc-43f3-e3bb-5d128f0f406a"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords corpus if not already downloaded\n",
        "# nltk.download('stopwords') # Already downloaded in initial setup\n",
        "\n",
        "# Define the set of English stopwords for efficient lookup\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from the NLTK-tokenized words\n",
        "nltk_filtered_words = []\n",
        "for sentence_words in nltk_words:\n",
        "    filtered_sentence = [word for word in sentence_words if word.lower() not in stop_words]\n",
        "    nltk_filtered_words.append(filtered_sentence)\n",
        "\n",
        "# Print the first few filtered sentences\n",
        "print(\"First 3 Sentences after Stopword Removal (NLTK):\")\n",
        "for i, filtered_sentence in enumerate(nltk_filtered_words[:3]):\n",
        "    print(f\"Sentence {i+1}: {filtered_sentence}\")\n",
        "    print(\"--------------------\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 Sentences after Stopword Removal (NLTK):\n",
            "Sentence 1: ['Type', '2', 'diabetes', 'chronic', 'condition', 'affects', 'way', 'body', 'processes', 'blood', 'sugar', '(', 'glucose', ')', '.']\n",
            "--------------------\n",
            "Sentence 2: ['type', '2', 'diabetes', ',', 'body', 'either', \"n't\", 'produce', 'enough', 'insulin', ',', 'resists', 'insulin', '.']\n",
            "--------------------\n",
            "Sentence 3: ['Symptoms', 'include', 'increased', 'thirst', ',', 'frequent', 'urination', ',', 'increased', 'hunger', ',', 'unintended', 'weight', 'loss', ',', 'fatigue', ',', 'blurred', 'vision', ',', 'slow-healing', 'sores', ',', 'frequent', 'infections', '.']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JvbzjjKKWJ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a8b1250",
        "outputId": "7b3e6d04-bf80-41dd-d27b-170ea91e2f2d"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords corpus if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the set of English stopwords for efficient lookup\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from the NLTK-tokenized words\n",
        "nltk_filtered_words = []\n",
        "for sentence_words in nltk_words:\n",
        "    filtered_sentence = [word for word in sentence_words if word.lower() not in stop_words]\n",
        "    nltk_filtered_words.append(filtered_sentence)\n",
        "\n",
        "# Print the first few filtered sentences\n",
        "print(\"First 3 Sentences after Stopword Removal (NLTK):\")\n",
        "for i, filtered_sentence in enumerate(nltk_filtered_words[:3]):\n",
        "    print(f\"Sentence {i+1}: {filtered_sentence}\")\n",
        "    print(\"--------------------\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 Sentences after Stopword Removal (NLTK):\n",
            "Sentence 1: ['Type', '2', 'diabetes', 'chronic', 'condition', 'affects', 'way', 'body', 'processes', 'blood', 'sugar', '(', 'glucose', ')', '.']\n",
            "--------------------\n",
            "Sentence 2: ['type', '2', 'diabetes', ',', 'body', 'either', \"n't\", 'produce', 'enough', 'insulin', ',', 'resists', 'insulin', '.']\n",
            "--------------------\n",
            "Sentence 3: ['Symptoms', 'include', 'increased', 'thirst', ',', 'frequent', 'urination', ',', 'increased', 'hunger', ',', 'unintended', 'weight', 'loss', ',', 'fatigue', ',', 'blurred', 'vision', ',', 'slow-healing', 'sores', ',', 'frequent', 'infections', '.']\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "537e9c99",
        "outputId": "5c10b7ef-ab48-4ee3-cbf3-bc8d48b699e5"
      },
      "source": [
        "## Remove Stopwords (spaCy)\n",
        "\n",
        "### Subtask:\n",
        "Remove stopwords from the spaCy-tokenized words, leveraging spaCy's linguistic capabilities."
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 4) (ipython-input-1523370938.py, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1523370938.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Remove stopwords from the spaCy-tokenized words, leveraging spaCy's linguistic capabilities.\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4e757a3",
        "outputId": "00fee4c8-9006-4887-e4c0-7acb62d86f4b"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Assuming 'nlp' model and 'doc' object from 'medical_text' are already loaded/processed\n",
        "# doc = nlp(medical_text) # This line is commented as doc is already available from previous steps\n",
        "\n",
        "# Remove stopwords and punctuation from the spaCy-tokenized words\n",
        "spacy_filtered_words = []\n",
        "for sent in doc.sents:\n",
        "    filtered_sentence = [token.text for token in sent if not token.is_stop and not token.is_punct]\n",
        "    spacy_filtered_words.append(filtered_sentence)\n",
        "\n",
        "# Print the first few filtered sentences\n",
        "print(\"First 3 Sentences after Stopword and Punctuation Removal (spaCy):\")\n",
        "for i, filtered_sentence in enumerate(spacy_filtered_words[:3]):\n",
        "    print(f\"Sentence {i+1}: {filtered_sentence}\")\n",
        "    print(\"--------------------\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 Sentences after Stopword and Punctuation Removal (spaCy):\n",
            "Sentence 1: ['\\n', 'Type', '2', 'diabetes', 'chronic', 'condition', 'affects', 'way', 'body', 'processes', 'blood', 'sugar', 'glucose']\n",
            "--------------------\n",
            "Sentence 2: ['type', '2', 'diabetes', 'body', 'produce', 'insulin', 'resists', 'insulin']\n",
            "--------------------\n",
            "Sentence 3: ['Symptoms', 'include', 'increased', 'thirst', 'frequent', 'urination', 'increased', 'hunger', 'unintended', 'weight', 'loss', 'fatigue', 'blurred', 'vision', 'slow', 'healing', 'sores', 'frequent', 'infections']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53845d20",
        "outputId": "59d2c469-1dff-471e-b436-6bf6491e6f9f"
      },
      "source": [
        "print(\"\\n--- Comparison for the first sentence ---\\n\")\n",
        "\n",
        "# Assuming the first sentence is selected for comparison\n",
        "sentence_index = 0\n",
        "\n",
        "# NLTK outputs\n",
        "print(f\"Original NLTK Tokens (Sentence {sentence_index + 1}): {nltk_words[sentence_index]}\")\n",
        "print(f\"NLTK Stemmed Words (Sentence {sentence_index + 1}): {nltk_stemmed_words[sentence_index]}\")\n",
        "print(f\"NLTK Lemmatized Words (Sentence {sentence_index + 1}): {nltk_lemmatized_words[sentence_index]}\")\n",
        "\n",
        "print(\"--------------------\")\n",
        "\n",
        "# spaCy outputs\n",
        "print(f\"Original spaCy Tokens (Sentence {sentence_index + 1}): {spacy_words[sentence_index]}\")\n",
        "print(f\"spaCy Lemmatized Words (Sentence {sentence_index + 1}): {spacy_lemmatized_words[sentence_index]}\")\n",
        "\n",
        "print(\"--------------------\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparison for the first sentence ---\n",
            "\n",
            "Original NLTK Tokens (Sentence 1): ['Type', '2', 'diabetes', 'is', 'a', 'chronic', 'condition', 'that', 'affects', 'the', 'way', 'the', 'body', 'processes', 'blood', 'sugar', '(', 'glucose', ')', '.']\n",
            "NLTK Stemmed Words (Sentence 1): ['type', '2', 'diabet', 'is', 'a', 'chronic', 'condit', 'that', 'affect', 'the', 'way', 'the', 'bodi', 'process', 'blood', 'sugar', '(', 'glucos', ')', '.']\n",
            "NLTK Lemmatized Words (Sentence 1): ['Type', '2', 'diabetes', 'is', 'a', 'chronic', 'condition', 'that', 'affect', 'the', 'way', 'the', 'body', 'process', 'blood', 'sugar', '(', 'glucose', ')', '.']\n",
            "--------------------\n",
            "Original spaCy Tokens (Sentence 1): ['\\n', 'Type', '2', 'diabetes', 'is', 'a', 'chronic', 'condition', 'that', 'affects', 'the', 'way', 'the', 'body', 'processes', 'blood', 'sugar', '(', 'glucose', ')', '.']\n",
            "spaCy Lemmatized Words (Sentence 1): ['\\n', 'Type', '2', 'diabetes', 'be', 'a', 'chronic', 'condition', 'that', 'affect', 'the', 'way', 'the', 'body', 'process', 'blood', 'sugar', '(', 'glucose', ')', '.']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRUniversity=\"\"\"The SR University campus is located in Ananthasagar village of Hasanparthy Mandal in Warangal, Telangana, India.\n",
        "It is in 150 acres, with both separate hostel facilities for boys and girls.\n",
        "There is a huge central library along with Indias largest Technology Business Incubator (TBI) in tier 2 cities.\"\"\""
      ],
      "metadata": {
        "id": "zxPfWcVpXVg3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRUniversity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "q8yottZTXbfr",
        "outputId": "580ceb40-a388-4f50-b0e3-8d3154e3c31f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The SR University campus is located in Ananthasagar village of Hasanparthy Mandal in Warangal, Telangana, India. \\nIt is in 150 acres, with both separate hostel facilities for boys and girls. \\nThere is a huge central library along with Indias largest Technology Business Incubator (TBI) in tier 2 cities.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(SRUniversity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDtk3BbVXi_E",
        "outputId": "05facc90-1c26-4d63-f825-0bb5f5ecf2ef"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'SR',\n",
              " 'University',\n",
              " 'campus',\n",
              " 'is',\n",
              " 'located',\n",
              " 'in',\n",
              " 'Ananthasagar',\n",
              " 'village',\n",
              " 'of',\n",
              " 'Hasanparthy',\n",
              " 'Mandal',\n",
              " 'in',\n",
              " 'Warangal',\n",
              " ',',\n",
              " 'Telangana',\n",
              " ',',\n",
              " 'India',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'in',\n",
              " '150',\n",
              " 'acres',\n",
              " ',',\n",
              " 'with',\n",
              " 'both',\n",
              " 'separate',\n",
              " 'hostel',\n",
              " 'facilities',\n",
              " 'for',\n",
              " 'boys',\n",
              " 'and',\n",
              " 'girls',\n",
              " '.',\n",
              " 'There',\n",
              " 'is',\n",
              " 'a',\n",
              " 'huge',\n",
              " 'central',\n",
              " 'library',\n",
              " 'along',\n",
              " 'with',\n",
              " 'Indias',\n",
              " 'largest',\n",
              " 'Technology',\n",
              " 'Business',\n",
              " 'Incubator',\n",
              " '(',\n",
              " 'TBI',\n",
              " ')',\n",
              " 'in',\n",
              " 'tier',\n",
              " '2',\n",
              " 'cities',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(SRUniversity)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc6Tt1d3XlCY",
        "outputId": "6e6054fa-ddfd-48bc-d391-494caf7d0b80"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The SR University campus is located in Ananthasagar village of Hasanparthy Mandal in Warangal, Telangana, India.',\n",
              " 'It is in 150 acres, with both separate hostel facilities for boys and girls.',\n",
              " 'There is a huge central library along with Indias largest Technology Business Incubator (TBI) in tier 2 cities.']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7EXcbygXqFi",
        "outputId": "2347ceea-397f-4a79-c723-a830128ca282"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote = word_tokenize(SRUniversity)\n",
        "words_in_quote\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI5I1ISTXu5u",
        "outputId": "bd340ceb-fe8d-4997-ab44-ac459d85cac5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'SR',\n",
              " 'University',\n",
              " 'campus',\n",
              " 'is',\n",
              " 'located',\n",
              " 'in',\n",
              " 'Ananthasagar',\n",
              " 'village',\n",
              " 'of',\n",
              " 'Hasanparthy',\n",
              " 'Mandal',\n",
              " 'in',\n",
              " 'Warangal',\n",
              " ',',\n",
              " 'Telangana',\n",
              " ',',\n",
              " 'India',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'in',\n",
              " '150',\n",
              " 'acres',\n",
              " ',',\n",
              " 'with',\n",
              " 'both',\n",
              " 'separate',\n",
              " 'hostel',\n",
              " 'facilities',\n",
              " 'for',\n",
              " 'boys',\n",
              " 'and',\n",
              " 'girls',\n",
              " '.',\n",
              " 'There',\n",
              " 'is',\n",
              " 'a',\n",
              " 'huge',\n",
              " 'central',\n",
              " 'library',\n",
              " 'along',\n",
              " 'with',\n",
              " 'Indias',\n",
              " 'largest',\n",
              " 'Technology',\n",
              " 'Business',\n",
              " 'Incubator',\n",
              " '(',\n",
              " 'TBI',\n",
              " ')',\n",
              " 'in',\n",
              " 'tier',\n",
              " '2',\n",
              " 'cities',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_list = []\n",
        "for word in words_in_quote:\n",
        "  if word.casefold() not in stop_words:\n",
        "    filtered_list.append(word)\n",
        "filtered_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs4che29X16w",
        "outputId": "0fc70d8b-7969-4ece-c187-f689e1abea35"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SR',\n",
              " 'University',\n",
              " 'campus',\n",
              " 'located',\n",
              " 'Ananthasagar',\n",
              " 'village',\n",
              " 'Hasanparthy',\n",
              " 'Mandal',\n",
              " 'Warangal',\n",
              " ',',\n",
              " 'Telangana',\n",
              " ',',\n",
              " 'India',\n",
              " '.',\n",
              " '150',\n",
              " 'acres',\n",
              " ',',\n",
              " 'separate',\n",
              " 'hostel',\n",
              " 'facilities',\n",
              " 'boys',\n",
              " 'girls',\n",
              " '.',\n",
              " 'huge',\n",
              " 'central',\n",
              " 'library',\n",
              " 'along',\n",
              " 'Indias',\n",
              " 'largest',\n",
              " 'Technology',\n",
              " 'Business',\n",
              " 'Incubator',\n",
              " '(',\n",
              " 'TBI',\n",
              " ')',\n",
              " 'tier',\n",
              " '2',\n",
              " 'cities',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "words = word_tokenize(SRUniversity)\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "stemmed_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCcKL279X6yb",
        "outputId": "bb47c86b-f01c-40c1-805c-9d468b39209c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'sr',\n",
              " 'univers',\n",
              " 'campu',\n",
              " 'is',\n",
              " 'locat',\n",
              " 'in',\n",
              " 'ananthasagar',\n",
              " 'villag',\n",
              " 'of',\n",
              " 'hasanparthi',\n",
              " 'mandal',\n",
              " 'in',\n",
              " 'warang',\n",
              " ',',\n",
              " 'telangana',\n",
              " ',',\n",
              " 'india',\n",
              " '.',\n",
              " 'it',\n",
              " 'is',\n",
              " 'in',\n",
              " '150',\n",
              " 'acr',\n",
              " ',',\n",
              " 'with',\n",
              " 'both',\n",
              " 'separ',\n",
              " 'hostel',\n",
              " 'facil',\n",
              " 'for',\n",
              " 'boy',\n",
              " 'and',\n",
              " 'girl',\n",
              " '.',\n",
              " 'there',\n",
              " 'is',\n",
              " 'a',\n",
              " 'huge',\n",
              " 'central',\n",
              " 'librari',\n",
              " 'along',\n",
              " 'with',\n",
              " 'india',\n",
              " 'largest',\n",
              " 'technolog',\n",
              " 'busi',\n",
              " 'incub',\n",
              " '(',\n",
              " 'tbi',\n",
              " ')',\n",
              " 'in',\n",
              " 'tier',\n",
              " '2',\n",
              " 'citi',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language='english')\n",
        "words = word_tokenize(SRUniversity)\n",
        "for word in words:\n",
        "    print(word,\"--->\",snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqYEiAGjX6gW",
        "outputId": "f1e87a50-e907-4d20-db4f-6c3196168089"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ---> the\n",
            "SR ---> sr\n",
            "University ---> univers\n",
            "campus ---> campus\n",
            "is ---> is\n",
            "located ---> locat\n",
            "in ---> in\n",
            "Ananthasagar ---> ananthasagar\n",
            "village ---> villag\n",
            "of ---> of\n",
            "Hasanparthy ---> hasanparthi\n",
            "Mandal ---> mandal\n",
            "in ---> in\n",
            "Warangal ---> warang\n",
            ", ---> ,\n",
            "Telangana ---> telangana\n",
            ", ---> ,\n",
            "India ---> india\n",
            ". ---> .\n",
            "It ---> it\n",
            "is ---> is\n",
            "in ---> in\n",
            "150 ---> 150\n",
            "acres ---> acr\n",
            ", ---> ,\n",
            "with ---> with\n",
            "both ---> both\n",
            "separate ---> separ\n",
            "hostel ---> hostel\n",
            "facilities ---> facil\n",
            "for ---> for\n",
            "boys ---> boy\n",
            "and ---> and\n",
            "girls ---> girl\n",
            ". ---> .\n",
            "There ---> there\n",
            "is ---> is\n",
            "a ---> a\n",
            "huge ---> huge\n",
            "central ---> central\n",
            "library ---> librari\n",
            "along ---> along\n",
            "with ---> with\n",
            "Indias ---> india\n",
            "largest ---> largest\n",
            "Technology ---> technolog\n",
            "Business ---> busi\n",
            "Incubator ---> incub\n",
            "( ---> (\n",
            "TBI ---> tbi\n",
            ") ---> )\n",
            "in ---> in\n",
            "tier ---> tier\n",
            "2 ---> 2\n",
            "cities ---> citi\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regexp = RegexpStemmer('ing|e', min=4)\n",
        "words = word_tokenize(SRUniversity)\n",
        "for word in words:\n",
        "    print(word,\"--->\",regexp.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOFFMdYaX6CX",
        "outputId": "b2174394-ebfe-4f99-afb8-0143ea55b32a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ---> The\n",
            "SR ---> SR\n",
            "University ---> Univrsity\n",
            "campus ---> campus\n",
            "is ---> is\n",
            "located ---> locatd\n",
            "in ---> in\n",
            "Ananthasagar ---> Ananthasagar\n",
            "village ---> villag\n",
            "of ---> of\n",
            "Hasanparthy ---> Hasanparthy\n",
            "Mandal ---> Mandal\n",
            "in ---> in\n",
            "Warangal ---> Warangal\n",
            ", ---> ,\n",
            "Telangana ---> Tlangana\n",
            ", ---> ,\n",
            "India ---> India\n",
            ". ---> .\n",
            "It ---> It\n",
            "is ---> is\n",
            "in ---> in\n",
            "150 ---> 150\n",
            "acres ---> acrs\n",
            ", ---> ,\n",
            "with ---> with\n",
            "both ---> both\n",
            "separate ---> sparat\n",
            "hostel ---> hostl\n",
            "facilities ---> facilitis\n",
            "for ---> for\n",
            "boys ---> boys\n",
            "and ---> and\n",
            "girls ---> girls\n",
            ". ---> .\n",
            "There ---> Thr\n",
            "is ---> is\n",
            "a ---> a\n",
            "huge ---> hug\n",
            "central ---> cntral\n",
            "library ---> library\n",
            "along ---> along\n",
            "with ---> with\n",
            "Indias ---> Indias\n",
            "largest ---> largst\n",
            "Technology ---> Tchnology\n",
            "Business ---> Businss\n",
            "Incubator ---> Incubator\n",
            "( ---> (\n",
            "TBI ---> TBI\n",
            ") ---> )\n",
            "in ---> in\n",
            "tier ---> tir\n",
            "2 ---> 2\n",
            "cities ---> citis\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = word_tokenize(SRUniversity)\n",
        "for word in words:\n",
        "    print(word,\"--->\",lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGn5prUAYNTx",
        "outputId": "fbad3fa8-a4d1-423c-8fee-03f104cde3df"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ---> The\n",
            "SR ---> SR\n",
            "University ---> University\n",
            "campus ---> campus\n",
            "is ---> is\n",
            "located ---> located\n",
            "in ---> in\n",
            "Ananthasagar ---> Ananthasagar\n",
            "village ---> village\n",
            "of ---> of\n",
            "Hasanparthy ---> Hasanparthy\n",
            "Mandal ---> Mandal\n",
            "in ---> in\n",
            "Warangal ---> Warangal\n",
            ", ---> ,\n",
            "Telangana ---> Telangana\n",
            ", ---> ,\n",
            "India ---> India\n",
            ". ---> .\n",
            "It ---> It\n",
            "is ---> is\n",
            "in ---> in\n",
            "150 ---> 150\n",
            "acres ---> acre\n",
            ", ---> ,\n",
            "with ---> with\n",
            "both ---> both\n",
            "separate ---> separate\n",
            "hostel ---> hostel\n",
            "facilities ---> facility\n",
            "for ---> for\n",
            "boys ---> boy\n",
            "and ---> and\n",
            "girls ---> girl\n",
            ". ---> .\n",
            "There ---> There\n",
            "is ---> is\n",
            "a ---> a\n",
            "huge ---> huge\n",
            "central ---> central\n",
            "library ---> library\n",
            "along ---> along\n",
            "with ---> with\n",
            "Indias ---> Indias\n",
            "largest ---> largest\n",
            "Technology ---> Technology\n",
            "Business ---> Business\n",
            "Incubator ---> Incubator\n",
            "( ---> (\n",
            "TBI ---> TBI\n",
            ") ---> )\n",
            "in ---> in\n",
            "tier ---> tier\n",
            "2 ---> 2\n",
            "cities ---> city\n",
            ". ---> .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sRzzwJmqYNNE",
        "outputId": "4483f93c-95a9-4f55-911d-d7ad3ce97270"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'worst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\", pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YHlpebJsYrqR",
        "outputId": "c14c443f-70bc-4ac9-acf6-f197d0b1d124"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer, WordNetLemmatizer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing|e', min=4)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\"]\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer','WordNetLemmatizer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word),lemmatizer.lemmatize(word)))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDfbsyT8YrhP",
        "outputId": "0c57d9ee-5116-4816-bf70-db38faf5285e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          WordNetLemmatizer                                 \n",
            "friend              friend              friend              friend                        frind                                   friend                                            \n",
            "friendship          friendship          friendship          friend                        frindship                               friendship                                        \n",
            "friends             friend              friend              friend                        frinds                                  friend                                            \n",
            "friendships         friendship          friendship          friend                        frindships                              friendship                                        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"NLP models are transforming the world rapidly!\"\"\""
      ],
      "metadata": {
        "id": "N1JVvmcHaMoN"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt_tab\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "words_sample_text = word_tokenize(medical_text) # Corrected: using medical_text instead of undefined sample_text\n",
        "print(f\"Word Tokens in sample text: \\n{words_sample_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnppjl12ZdG7",
        "outputId": "0f627498-2bfb-4bb8-e290-0141f8e84ba4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens in sample text: \n",
            "['Type', '2', 'diabetes', 'is', 'a', 'chronic', 'condition', 'that', 'affects', 'the', 'way', 'the', 'body', 'processes', 'blood', 'sugar', '(', 'glucose', ')', '.', 'With', 'type', '2', 'diabetes', ',', 'the', 'body', 'either', 'does', \"n't\", 'produce', 'enough', 'insulin', ',', 'or', 'it', 'resists', 'insulin', '.', 'Symptoms', 'can', 'include', 'increased', 'thirst', ',', 'frequent', 'urination', ',', 'increased', 'hunger', ',', 'unintended', 'weight', 'loss', ',', 'fatigue', ',', 'blurred', 'vision', ',', 'slow-healing', 'sores', ',', 'and', 'frequent', 'infections', '.', 'Long-term', 'complications', 'can', 'include', 'heart', 'disease', ',', 'stroke', ',', 'kidney', 'disease', ',', 'nerve', 'damage', ',', 'and', 'eye', 'damage', '.', 'Management', 'often', 'involves', 'lifestyle', 'changes', 'such', 'as', 'diet', 'and', 'exercise', ',', 'and', 'sometimes', 'medication', 'or', 'insulin', 'therapy', '.', 'Regular', 'monitoring', 'of', 'blood', 'glucose', 'levels', 'is', 'crucial', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words_sample_text = [stemmer.stem(words) for words in words_sample_text]\n",
        "print(f\"Stemmed Words in Sample Text: \\n{stemmed_words_sample_text}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jahyV9K_aPc9",
        "outputId": "615160ff-089d-4156-9601-e28ea994a23f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words in Sample Text: \n",
            "['type', '2', 'diabet', 'is', 'a', 'chronic', 'condit', 'that', 'affect', 'the', 'way', 'the', 'bodi', 'process', 'blood', 'sugar', '(', 'glucos', ')', '.', 'with', 'type', '2', 'diabet', ',', 'the', 'bodi', 'either', 'doe', \"n't\", 'produc', 'enough', 'insulin', ',', 'or', 'it', 'resist', 'insulin', '.', 'symptom', 'can', 'includ', 'increas', 'thirst', ',', 'frequent', 'urin', ',', 'increas', 'hunger', ',', 'unintend', 'weight', 'loss', ',', 'fatigu', ',', 'blur', 'vision', ',', 'slow-heal', 'sore', ',', 'and', 'frequent', 'infect', '.', 'long-term', 'complic', 'can', 'includ', 'heart', 'diseas', ',', 'stroke', ',', 'kidney', 'diseas', ',', 'nerv', 'damag', ',', 'and', 'eye', 'damag', '.', 'manag', 'often', 'involv', 'lifestyl', 'chang', 'such', 'as', 'diet', 'and', 'exercis', ',', 'and', 'sometim', 'medic', 'or', 'insulin', 'therapi', '.', 'regular', 'monitor', 'of', 'blood', 'glucos', 'level', 'is', 'crucial', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words_sample_text = [lemmatizer.lemmatize(words) for words in words_sample_text]\n",
        "print(f\"Lemmatized Words in Sample Text: \\n{lemmatized_words_sample_text}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFLteDvcaPRP",
        "outputId": "5873e591-cb8d-4c6e-cb89-9d794f50e6fa"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words in Sample Text: \n",
            "['Type', '2', 'diabetes', 'is', 'a', 'chronic', 'condition', 'that', 'affect', 'the', 'way', 'the', 'body', 'process', 'blood', 'sugar', '(', 'glucose', ')', '.', 'With', 'type', '2', 'diabetes', ',', 'the', 'body', 'either', 'doe', \"n't\", 'produce', 'enough', 'insulin', ',', 'or', 'it', 'resists', 'insulin', '.', 'Symptoms', 'can', 'include', 'increased', 'thirst', ',', 'frequent', 'urination', ',', 'increased', 'hunger', ',', 'unintended', 'weight', 'loss', ',', 'fatigue', ',', 'blurred', 'vision', ',', 'slow-healing', 'sore', ',', 'and', 'frequent', 'infection', '.', 'Long-term', 'complication', 'can', 'include', 'heart', 'disease', ',', 'stroke', ',', 'kidney', 'disease', ',', 'nerve', 'damage', ',', 'and', 'eye', 'damage', '.', 'Management', 'often', 'involves', 'lifestyle', 'change', 'such', 'a', 'diet', 'and', 'exercise', ',', 'and', 'sometimes', 'medication', 'or', 'insulin', 'therapy', '.', 'Regular', 'monitoring', 'of', 'blood', 'glucose', 'level', 'is', 'crucial', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}