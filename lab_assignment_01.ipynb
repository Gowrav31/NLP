{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2pqLIEHBt/umXfnrnPzWa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gowrav31/NLP/blob/main/lab_assignment_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC0nV4tH2g5C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed2af8e0"
      },
      "source": [
        "# Task\n",
        "Perform text preprocessing on a short paragraph of text by loading it, counting the number of words, and converting the text to lowercase, then summarize the results including the word count and the lowercase version of the text. This will involve installing and importing the NLTK and spaCy libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1f50ee1"
      },
      "source": [
        "## Install and Import Libraries\n",
        "\n",
        "### Subtask:\n",
        "Install NLTK and spaCy, and then import them along with any necessary models or data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2b9bcfa"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to install the `nltk` library as per the instructions, which is the first step in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96518062",
        "outputId": "5c40ab24-be36-482f-ce5a-676ef0acb311"
      },
      "source": [
        "print(\"Installing NLTK...\")\n",
        "!pip install nltk\n",
        "print(\"NLTK installed successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing NLTK...\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "NLTK installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f07f6f51"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that NLTK is installed, I need to import it as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f647d608",
        "outputId": "de3e3365-f149-4916-90df-40fd629bd363"
      },
      "source": [
        "import nltk\n",
        "print(\"NLTK imported successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "520d1f99"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the subtask instructions, I need to download the 'punkt' tokenizer models from NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8ed7ae9",
        "outputId": "79df310f-f940-4151-8279-a00c3e1adcd2"
      },
      "source": [
        "print(\"Downloading NLTK 'punkt' tokenizer...\")\n",
        "nltk.download('punkt')\n",
        "print(\"NLTK 'punkt' tokenizer downloaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK 'punkt' tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK 'punkt' tokenizer downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cf5ad76"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the subtask instructions, I now need to install the `spacy` library using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66f0e7f5",
        "outputId": "a1f0c50b-138f-4f34-bd19-40078bf0f326"
      },
      "source": [
        "print(\"Installing spaCy...\")\n",
        "!pip install spacy\n",
        "print(\"spaCy installed successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing spaCy...\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "spaCy installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adf0629e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that spaCy is installed, I need to download a small English language model ('en_core_web_sm') using spaCy's command-line interface, as specified in the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7abad7d3",
        "outputId": "eabf97ae-6824-45f9-bf0e-1d551042475a"
      },
      "source": [
        "print(\"Downloading spaCy English model 'en_core_web_sm'...\")\n",
        "!python -m spacy download en_core_web_sm\n",
        "print(\"spaCy English model 'en_core_web_sm' downloaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading spaCy English model 'en_core_web_sm'...\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "spaCy English model 'en_core_web_sm' downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8ecf728"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the spaCy English model has been downloaded, the next step in the subtask is to import the `spacy` library into the environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9d70f05",
        "outputId": "3cb1899e-2a0a-49df-8b90-cbb7522d3dbe"
      },
      "source": [
        "import spacy\n",
        "print(\"spaCy imported successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faa3af5f"
      },
      "source": [
        "## Load Text\n",
        "\n",
        "### Subtask:\n",
        "Load a short paragraph of your choice into a Python string variable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66fed1b9"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to create a Python string variable `text_paragraph` and assign a short paragraph to it, as instructed by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75dc381a",
        "outputId": "2adb8dbb-7be6-423a-e677-205f79b49ae6"
      },
      "source": [
        "text_paragraph = \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables computers to understand, interpret, and generate human-like text.\"\n",
        "print(\"Text paragraph loaded successfully:\")\n",
        "print(text_paragraph)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text paragraph loaded successfully:\n",
            "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables computers to understand, interpret, and generate human-like text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcfe585f"
      },
      "source": [
        "## Count Words\n",
        "\n",
        "### Subtask:\n",
        "Count the number of words in the loaded text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74792529"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to tokenize the `text_paragraph` using `nltk.word_tokenize()` and then count the number of tokens to get the word count, as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e15c85c",
        "outputId": "56da03f9-09fa-4e1b-d87a-9073021cf9c6"
      },
      "source": [
        "print(\"Tokenizing text and counting words...\")\n",
        "words = nltk.word_tokenize(text_paragraph)\n",
        "word_count = len(words)\n",
        "print(f\"The text contains {word_count} words.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing text and counting words...\n",
            "The text contains 36 words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34682e2a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous cell failed because `punkt_tab` resource was not found by NLTK, even though `punkt` was downloaded. The traceback suggests downloading `punkt_tab`. I will add a step to download this resource using `nltk.download('punkt_tab')` before retrying the word tokenization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1ab0313",
        "outputId": "7eb94738-72ab-41ec-e0fb-c90cce3bfc1b"
      },
      "source": [
        "print(\"Downloading NLTK 'punkt' and 'punkt_tab' tokenizers...\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "print(\"NLTK 'punkt' and 'punkt_tab' tokenizers downloaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK 'punkt' and 'punkt_tab' tokenizers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK 'punkt' and 'punkt_tab' tokenizers downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c03af322"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous fix successfully downloaded the `punkt_tab` resource for NLTK, which was the cause of the `LookupError`. Now that the necessary resources are available, I can re-run the word counting code to complete the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d37ccdc4",
        "outputId": "48992dbb-9f00-421a-a558-7debe2f7bdc4"
      },
      "source": [
        "print(\"Tokenizing text and counting words...\")\n",
        "words = nltk.word_tokenize(text_paragraph)\n",
        "word_count = len(words)\n",
        "print(f\"The text contains {word_count} words.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing text and counting words...\n",
            "The text contains 36 words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e0a31ff"
      },
      "source": [
        "## Convert to Lowercase\n",
        "\n",
        "### Subtask:\n",
        "Convert the entire text to lowercase using Python's built-in functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1d8f811"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to convert the `text_paragraph` to lowercase using the `.lower()` method and store it in a new variable `text_paragraph_lower`, then print the result as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b05f559d",
        "outputId": "e15c7daf-2445-41da-bd2a-787909b51b43"
      },
      "source": [
        "print(\"Converting text to lowercase...\")\n",
        "text_paragraph_lower = text_paragraph.lower()\n",
        "print(\"Lowercase text:\")\n",
        "print(text_paragraph_lower)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting text to lowercase...\n",
            "Lowercase text:\n",
            "natural language processing (nlp) is a field of artificial intelligence that focuses on the interaction between computers and human language. it enables computers to understand, interpret, and generate human-like text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ed4b58"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the results of the preprocessing steps, including the word count and the lowercase version of the text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1153b24c"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The NLTK and spaCy libraries, along with their respective language models (`punkt` for NLTK and `en_core_web_sm` for spaCy), were successfully installed and imported.\n",
        "*   A sample paragraph was loaded: \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables computers to understand, interpret, and generate human-like text.\"\n",
        "*   The word count for the loaded text was determined to be 36 words using NLTK's `word_tokenize` function.\n",
        "*   The text was successfully converted to its lowercase version: \"natural language processing (nlp) is a field of artificial intelligence that focuses on the interaction between computers and human language. it enables computers to understand, interpret, and generate human-like text.\"\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The foundational preprocessing steps have been completed, making the text ready for more advanced NLP tasks such as tokenization, part-of-speech tagging, or entity recognition using the installed libraries.\n",
        "*   For a more comprehensive preprocessing pipeline, consider incorporating additional steps like punctuation removal, stop-word filtering, and stemming or lemmatization to further refine the text for specific analytical goals.\n"
      ]
    }
  ]
}